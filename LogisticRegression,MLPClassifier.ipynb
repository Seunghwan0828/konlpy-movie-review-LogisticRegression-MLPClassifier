{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in c:\\users\\jshwa\\anaconda3\\lib\\site-packages (0.5.1)\n",
      "Requirement already satisfied: JPype1>=0.5.7 in c:\\users\\jshwa\\anaconda3\\lib\\site-packages (from konlpy) (0.7.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: gensim in c:\\users\\jshwa\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in c:\\users\\jshwa\\anaconda3\\lib\\site-packages (from gensim) (1.2.1)\n",
      "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in c:\\users\\jshwa\\anaconda3\\lib\\site-packages (from gensim) (1.8.4)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5.0 in c:\\users\\jshwa\\anaconda3\\lib\\site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in c:\\users\\jshwa\\anaconda3\\lib\\site-packages (from gensim) (1.16.4)\n",
      "Requirement already satisfied, skipping upgrade: boto>=2.32 in c:\\users\\jshwa\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in c:\\users\\jshwa\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: boto3 in c:\\users\\jshwa\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.9.238)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in c:\\users\\jshwa\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2019.6.16)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\jshwa\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (1.24.2)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in c:\\users\\jshwa\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in c:\\users\\jshwa\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.13.0,>=1.12.238 in c:\\users\\jshwa\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (1.12.238)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in c:\\users\\jshwa\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.4)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.3.0,>=0.2.0 in c:\\users\\jshwa\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (0.2.1)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in c:\\users\\jshwa\\anaconda3\\lib\\site-packages (from botocore<1.13.0,>=1.12.238->boto3->smart-open>=1.8.1->gensim) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in c:\\users\\jshwa\\anaconda3\\lib\\site-packages (from botocore<1.13.0,>=1.12.238->boto3->smart-open>=1.8.1->gensim) (0.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: utils in c:\\users\\jshwa\\anaconda3\\lib\\site-packages (0.9.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\jshwa\\anaconda3\\lib\\site-packages (3.4.4)\n",
      "Requirement already satisfied: six in c:\\users\\jshwa\\anaconda3\\lib\\site-packages (from nltk) (1.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jshwa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import font_manager, rc\n",
    "font_fname = 'c:/windows/fonts/gulim.ttc'     # A font of your choice\n",
    "font_name = font_manager.FontProperties(fname=font_fname).get_name()\n",
    "rc('font', family=font_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    with open(filename, 'r', encoding='UTF8') as f:\n",
    "        data = [line.split('\\t') for line in f.read().splitlines()]\n",
    "        data = data[1:]   # header 제외\n",
    "    return data\n",
    "train_data = read_data('C:\\\\Users\\\\jshwa\\\\Downloads\\\\ratings_train.txt')\n",
    "test_data = read_data('C:\\\\Users\\\\jshwa\\\\Downloads\\\\ratings_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150000\n",
      "3\n",
      "50000\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))      # nrows: 150000\n",
    "print(len(train_data[0]))   # ncols: 3\n",
    "print(len(test_data))       # nrows: 50000\n",
    "print(len(test_data[0]))     # ncols: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jshwa\\Anaconda3\\lib\\site-packages\\jpype\\_core.py:210: UserWarning: \n",
      "-------------------------------------------------------------------------------\n",
      "Deprecated: convertStrings was not specified when starting the JVM. The default\n",
      "behavior in JPype will be False starting in JPype 0.8. The recommended setting\n",
      "for new code is convertStrings=False.  The legacy value of True was assumed for\n",
      "this session. If you are a user of an application that reported this warning,\n",
      "please file a ticket with the developer.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "  \"\"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 468.196429s\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "import konlpy\n",
    "from konlpy.tag import Okt\n",
    "from konlpy.utils import pprint\n",
    "pos_tagger = Okt()\n",
    "def tokenize(doc):\n",
    "    # norm, stem은 optional\n",
    "    return ['/'.join(t) for t in pos_tagger.pos(doc, norm=True, stem=True)]\n",
    "train_docs = [(tokenize(row[1]), row[2]) for row in train_data]\n",
    "test_docs = [(tokenize(row[1]), row[2]) for row in test_data]\n",
    "\n",
    "end = time()\n",
    "print('Time: {:f}s'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['아/Exclamation',\n",
      "  '더빙/Noun',\n",
      "  '../Punctuation',\n",
      "  '진짜/Noun',\n",
      "  '짜증나다/Adjective',\n",
      "  '목소리/Noun'],\n",
      " '0')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(train_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [t for d in train_docs for t in d[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Text: NMSC>\n"
     ]
    }
   ],
   "source": [
    "text = nltk.Text(tokens, name='NMSC')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2159921\n",
      "49895\n",
      "[('./Punctuation', 67778),\n",
      " ('영화/Noun', 50818),\n",
      " ('하다/Verb', 41209),\n",
      " ('이/Josa', 38540),\n",
      " ('보다/Verb', 38538),\n",
      " ('의/Josa', 30188),\n",
      " ('../Punctuation', 29055),\n",
      " ('가/Josa', 26627),\n",
      " ('에/Josa', 26468),\n",
      " ('을/Josa', 23118)]\n"
     ]
    }
   ],
   "source": [
    "print(len(text.tokens))\n",
    "print(len(set(text.tokens)))\n",
    "pprint(text.vocab().most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text.plot(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "TaggedDocument = namedtuple('TaggedDocument', 'words tags')\n",
    "\n",
    "\n",
    "tagged_train_docs = [TaggedDocument(d, [c]) for d, c in train_docs]\n",
    "tagged_test_docs = [TaggedDocument(d, [c]) for d, c in test_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import doc2vec\n",
    "# 사전 구축\n",
    "doc_vectorizer = doc2vec.Doc2Vec(vector_size=500, alpha=0.025, min_alpha=0.025, seed=1234)\n",
    "doc_vectorizer.build_vocab(tagged_train_docs)\n",
    "# Train document vectors!\n",
    "for epoch in range(10):\n",
    "    doc_vectorizer.train(tagged_train_docs,epochs=10,total_examples=doc_vectorizer.corpus_count)\n",
    "    doc_vectorizer.alpha -= 0.002  # decrease the learning rate\n",
    "    doc_vectorizer.min_alpha = doc_vectorizer.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = [doc_vectorizer.infer_vector(doc.words) for doc in tagged_train_docs]\n",
    "train_y = [doc.tags[0] for doc in tagged_train_docs]\n",
    "test_x = [doc_vectorizer.infer_vector(doc.words) for doc in tagged_test_docs]\n",
    "test_y = [doc.tags[0] for doc in tagged_test_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6622"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state=1234)\n",
    "classifier.fit(train_x, train_y)\n",
    "classifier.score(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier(\n",
    "    hidden_layer_sizes=(50,),\n",
    "    max_iter=10,\n",
    "    alpha=1e-4,\n",
    "    solver='adam',\n",
    "    verbose=10,\n",
    "    tol=1e-4,\n",
    "    random_state=1,\n",
    "    learning_rate_init=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.59932523\n",
      "Iteration 2, loss = 0.57441135\n",
      "Iteration 3, loss = 0.56507552\n",
      "Iteration 4, loss = 0.56038985\n",
      "Iteration 5, loss = 0.55555834\n",
      "Iteration 6, loss = 0.55332870\n",
      "Iteration 7, loss = 0.55048922\n",
      "Iteration 8, loss = 0.54734600\n",
      "Iteration 9, loss = 0.54508699\n",
      "Iteration 10, loss = 0.54247497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jshwa\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 정확도: 0.681\n",
      "Time: 15.823843s\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "model.fit(train_x, train_y)\n",
    "y_pred = model.predict(test_x)\n",
    "acc = accuracy_score(y_pred, test_y)\n",
    "end = time()\n",
    "print(\"테스트 정확도: {:.3f}\".format(acc))\n",
    "print('Time: {:f}s'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "5주차 과제.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
